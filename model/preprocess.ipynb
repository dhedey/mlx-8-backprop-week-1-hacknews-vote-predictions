{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bdf05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4afb698d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:36:16.455502Z",
     "start_time": "2025-06-12T09:34:20.119689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidedey/NotWork/mlx-8-backprop-week-1-hacknews-vote-predictions/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from huggingface...\n",
      "Dataset lazily loaded, size: 4010957\n",
      "Dataset loaded into memory\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "print(\"Loading dataset from huggingface...\")\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "# https://huggingface.co/datasets/julien040/hacker-news-posts\n",
    "# NOTE: It only has a training split, so use that. And then we can divide it up into train/test ourselves\n",
    "dataset = load_dataset(\"julien040/hacker-news-posts\", split=\"train\")\n",
    "print(f\"Dataset lazily loaded, size: {len(dataset)}\")\n",
    "\n",
    "dataset = [d for d in dataset]\n",
    "print(f\"Dataset loaded into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c9319fd1c292c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:38:48.339056Z",
     "start_time": "2025-06-12T09:38:46.918011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing score: 0\n",
      "Negative score: 1\n",
      "Zero score: 1319\n",
      "Positive score: 4009637\n",
      "----\n",
      "None title: 0\n",
      "Empty title: 0\n",
      "Has title: 4010957\n",
      "----\n",
      "None url: 243946\n",
      "Empty url: 0\n",
      "Has url: 3767011\n"
     ]
    }
   ],
   "source": [
    "print(f\"Missing score: {sum(1 for d in dataset if d[\"score\"] is None)}\")\n",
    "print(f\"Negative score: {sum(1 for d in dataset if d[\"score\"] < 0)}\")\n",
    "print(f\"Zero score: {sum(1 for d in dataset if d[\"score\"] == 0)}\")\n",
    "print(f\"Positive score: {sum(1 for d in dataset if d[\"score\"] > 0)}\")\n",
    "print(\"----\")\n",
    "print(f\"None title: {sum(1 for d in dataset if d[\"title\"] is None)}\")\n",
    "print(f\"Empty title: {sum(1 for d in dataset if d[\"title\"] == \"\")}\")\n",
    "print(f\"Has title: {sum(1 for d in dataset if isinstance(d[\"title\"], str) and d[\"title\"] != \"\")}\")\n",
    "print(\"----\")\n",
    "print(f\"None url: {sum(1 for d in dataset if d[\"url\"] is None)}\")\n",
    "print(f\"Empty url: {sum(1 for d in dataset if d[\"url\"] == \"\")}\")\n",
    "print(f\"Has url: {sum(1 for d in dataset if isinstance(d[\"url\"], str) and d[\"url\"] != \"\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd4d970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T10:13:52.745218Z",
     "start_time": "2025-06-12T10:13:41.070239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset has length (3765851\n",
      "Dataset split into train (3012681), validation (376585) and test (376585\n"
     ]
    }
   ],
   "source": [
    "import tldextract\n",
    "import datasets\n",
    "\n",
    "# Extract domains\n",
    "def extract_domain(url):\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'https://' + url\n",
    "\n",
    "    extracted = tldextract.extract(url)\n",
    "\n",
    "    # Return domain.suffix (e.g., 'google.com', 'blogspot.com')\n",
    "    if extracted.domain and extracted.suffix:\n",
    "        return f\"{extracted.domain}.{extracted.suffix}\".lower()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "cleaned_dataset = dataset.map()\n",
    "\n",
    "for d in dataset:\n",
    "    if not isinstance(d[\"score\"], int):\n",
    "        continue\n",
    "    score = d[\"score\"]\n",
    "\n",
    "    if score < 0:\n",
    "        continue\n",
    "\n",
    "    if not isinstance(d[\"title\"], str):\n",
    "        continue\n",
    "    title = d[\"title\"]\n",
    "\n",
    "    if not isinstance(d[\"url\"], str):\n",
    "        continue\n",
    "    url = d[\"url\"]\n",
    "    domain = extract_domain(url)\n",
    "    if domain is None:\n",
    "        continue\n",
    "\n",
    "    cleaned_dataset.append({\n",
    "        \"id\": d[\"id\"],\n",
    "        \"author\": d[\"author\"],\n",
    "        \"title\": title,\n",
    "        \"domain\": domain,\n",
    "        \"time\": d[\"time\"],\n",
    "        \"score\": score,\n",
    "    })\n",
    "\n",
    "print(f\"Original dataset size: {len(cleaned_dataset)}\")\n",
    "print(f\"Cleaned dataset size: {len(cleaned_dataset)}\")\n",
    "print(f\"The cleaned dataset has titles, domains and non-negative scores\")\n",
    "\n",
    "\n",
    "(train_dataset, validation_dataset, test_dataset) = torch.utils.data.random_split(\n",
    "    cleaned_dataset,\n",
    "    [0.8, 0.1, 0.1],\n",
    "    torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Dataset split into train ({len(train_dataset)}), validation ({len(validation_dataset)}) and test ({len(test_dataset)}\")\n",
    "\n",
    "dataset = datasets.DatasetDict(\n",
    "\n",
    "){\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n",
    "\n",
    "dataset.push_to_hub(\"hacker-news-posts-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06bf73857090b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T11:12:37.202072Z",
     "start_time": "2025-06-12T11:12:37.194102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting column \"a\"\n",
      "- 3 distinct values across 8 entries\n",
      "- Keeping at least 80% of values. This corresponds to values with counts >= 3 (88% of total)\n",
      "- Kept 2 distinct values covering 7 entries\n",
      "- Saved to a_counts.csv\n"
     ]
    }
   ],
   "source": [
    "def save_frequencies(dataset, column: str, keep_most_frequent_proportion: float = 1, save = True):\n",
    "    frequencies = {}\n",
    "    for d in dataset:\n",
    "        value = d[column]\n",
    "        if value in frequencies:\n",
    "            frequencies[value] += 1\n",
    "        else:\n",
    "            frequencies[value] = 1\n",
    "\n",
    "    counts_df = pd.DataFrame(\n",
    "        {\n",
    "            column: frequencies.keys(),\n",
    "            \"count\": frequencies.values(),\n",
    "        }\n",
    "    )\n",
    "    counts_df.set_index(\"count\", inplace=True)\n",
    "    counts_df.sort_values(\"count\", ascending=False, inplace=True)\n",
    "    original_keys = len(counts_df.index)\n",
    "    original_count = counts_df[\"count\"].sum()\n",
    "    cut_off = counts_df[\"count\"].quantile(1-keep_most_frequent_proportion, interpolation=\"higher\")\n",
    "    filtered_df = counts_df[counts_df[\"count\"] >= cut_off]\n",
    "    filtered_keys = len(filtered_df.index)\n",
    "    filtered_count = filtered_df[\"count\"].sum()\n",
    "\n",
    "    file = f\"{column}_counts.csv\"\n",
    "    print(f\"Extracting column \\\"{column}\\\"\")\n",
    "    print(f\"- {original_keys} distinct values across {original_count} entries\")\n",
    "    print(f\"- Keeping at least {keep_most_frequent_proportion:.0%} of values. This corresponds to values with counts >= {cut_off} ({filtered_count/original_count:.0%} of total)\")\n",
    "    print(f\"- Kept {filtered_keys} distinct values covering {filtered_count} entries\")\n",
    "\n",
    "    if save:\n",
    "        print(f\"- Saved to {file}\")\n",
    "        counts_df.to_csv(file, index=False)\n",
    "\n",
    "save_frequencies([\n",
    "    { \"a\": \"hello\" },\n",
    "    { \"a\": \"hello\" },\n",
    "    { \"a\": \"hello\" },\n",
    "    { \"a\": \"hello\" },\n",
    "    { \"a\": \"world\" },\n",
    "    { \"a\": \"world\" },\n",
    "    { \"a\": \"world\" },\n",
    "    { \"a\": \"bob\" },\n",
    "], \"a\", 0.8, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "962b187b75f909fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T11:18:09.893307Z",
     "start_time": "2025-06-12T11:12:55.142497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading / importing dataset into memory (max 8GB)...\n",
      "Extracting column \"domain\"\n",
      "- 350516 distinct values across 3765851 entries\n",
      "- Keeping at least 80% of values. This corresponds to values with counts >= 1 (100% of total)\n",
      "- Kept 350516 distinct values covering 3765851 entries\n",
      "- Saved to domain_counts.csv\n",
      "Extracting column \"author\"\n",
      "- 320987 distinct values across 3765851 entries\n",
      "- Keeping at least 80% of values. This corresponds to values with counts >= 1 (100% of total)\n",
      "- Kept 320987 distinct values covering 3765851 entries\n",
      "- Saved to author_counts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 3765851/3765851 [00:13<00:00, 285516.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Downloading / importing dataset into memory (max 8GB)...\")\n",
    "\n",
    "datasets.config.IN_MEMORY_MAX_SIZE = 8 * 1024 * 1024 # 8GB\n",
    "\n",
    "dataset = datasets.load_dataset(\"julien040/hacker-news-posts\", split=\"train\")\n",
    "\n",
    "def filter_map(dataset: datasets.DatasetDict, map_or_none) -> datasets.DatasetDict:\n",
    "    return dataset.map(map_or_none).filter(lambda x: x is not None)\n",
    "\n",
    "# Extract domains\n",
    "def extract_domain(url):\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'https://' + url\n",
    "\n",
    "    extracted = tldextract.extract(url)\n",
    "\n",
    "    # Return domain.suffix (e.g., 'google.com', 'blogspot.com')\n",
    "    if extracted.domain and extracted.suffix:\n",
    "        return f\"{extracted.domain}.{extracted.suffix}\".lower()\n",
    "\n",
    "    return None\n",
    "\n",
    "def map_item(d):\n",
    "    if not isinstance(d[\"score\"], int):\n",
    "        return None\n",
    "    score = d[\"score\"]\n",
    "\n",
    "    if score < 0:\n",
    "        return None\n",
    "\n",
    "    if not isinstance(d[\"title\"], str):\n",
    "        return None\n",
    "    title = d[\"title\"]\n",
    "\n",
    "    if not isinstance(d[\"url\"], str):\n",
    "        return None\n",
    "    url = d[\"url\"]\n",
    "    domain = extract_domain(url)\n",
    "    if domain is None:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"id\": d[\"id\"],\n",
    "        \"author\": d[\"author\"],\n",
    "        \"title\": title,\n",
    "        \"domain\": domain,\n",
    "        \"time\": d[\"time\"],\n",
    "        \"score\": score,\n",
    "    }\n",
    "\n",
    "filtered_dataset = filter_map(dataset, map_item)\n",
    "\n",
    "save_frequencies(filtered_dataset, \"domain\", keep_most_frequent_proportion=0.8)\n",
    "save_frequencies(filtered_dataset, \"author\", keep_most_frequent_proportion=0.8)\n",
    "\n",
    "filtered_dataset.save_to_disk(\"filtered_dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
